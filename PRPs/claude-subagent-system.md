name: "Claude Code Sub-Agent System for Product Development"
description: |

## Purpose
Build a production-ready Claude Code system that automatically generates specialized sub-agents for each stage of the product development lifecycle, enabling seamless collaboration through structured handoffs and maintaining context across the entire development workflow.

## Core Principles
1. **Context is King**: Maintain comprehensive context preservation across all agent handoffs
2. **Validation Loops**: Implement quality gates at each stage transition with executable validation
3. **Orchestration Excellence**: Design hierarchical agent architecture with supervisor-worker patterns
4. **Progressive Delivery**: Enable parallel execution where workflow allows with proper synchronization
5. **Global rules**: Follow all rules in CLAUDE.md for consistent code structure and testing

---

## Goal
Create a comprehensive multi-agent orchestration system where users can initiate complex product development workflows through natural language, and Claude Code automatically generates and coordinates specialized sub-agents (Discovery, Specification, Design, Implementation, Validation, Deployment) that work together seamlessly while preserving context and maintaining quality gates throughout the entire development lifecycle.

## Why
- **Business value**: Reduce development cycle time by 40% through automated agent orchestration
- **Integration**: Enable seamless handoffs between development phases without context loss
- **Problems solved**: Eliminates manual context switching, improves deliverable consistency, scales development capabilities
- **User impact**: Provides domain-specific expertise at each workflow stage with automated quality validation

## What
A Claude Code system that:
- Analyzes project descriptions and automatically generates appropriate agent sequences
- Maintains persistent context across all agent handoffs with versioning and rollback capabilities
- Implements quality gates and validation checkpoints at each stage transition
- Supports parallel agent execution with resource coordination and conflict resolution
- Integrates with external development tools (GitHub, Figma, Jira, CI/CD pipelines)
- Provides real-time progress visibility and interactive approval workflows

### Success Criteria
- [ ] System successfully generates specialized agents based on workflow stage requirements
- [ ] Context preservation accuracy across handoffs exceeds 99%
- [ ] Quality gate validation prevents faulty stage transitions
- [ ] Parallel agent execution reduces overall development time by 40%
- [ ] External tool integrations synchronize artifacts automatically
- [ ] User satisfaction with agent-generated deliverables exceeds 4.5/5
- [ ] All tests pass with 80%+ coverage and code meets quality standards

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- url: https://ai.pydantic.dev/multi-agent-applications/
  why: Multi-agent system patterns, agent-as-tool orchestration, graph-based workflows
  
- url: https://www.anthropic.com/engineering/built-multi-agent-research-system
  why: Anthropic's multi-agent research system with context preservation patterns
  
- url: https://ai.pydantic.dev/agents/
  why: Core agent creation patterns, dependencies, and tool registration
  
- url: https://github.com/pydantic/pydantic-ai
  why: Latest Pydantic AI framework patterns and examples
  
- file: use-cases/pydantic-ai/examples/main_agent_reference/research_agent.py
  why: Multi-agent coordination pattern with agent-as-tool implementation
  
- file: use-cases/pydantic-ai/examples/main_agent_reference/models.py
  why: Structured data models for agent communication and validation
  
- file: use-cases/pydantic-ai/examples/main_agent_reference/providers.py
  why: Multi-provider LLM configuration with flexible model selection
  
- file: PRPs/EXAMPLE_multi_agent_prp.md
  why: Proven multi-agent system implementation blueprint and validation patterns
  
- url: https://docs.github.com/en/rest/repos/repos
  why: GitHub API integration for repository management and code synchronization
  
- url: https://www.figma.com/developers/api
  why: Figma API for design artifact synchronization and collaboration
  
- url: https://developer.atlassian.com/cloud/jira/platform/rest/v3/
  why: Jira API for project management and task tracking integration
```

### Current Codebase tree
```bash
.
â”œâ”€â”€ use-cases/
â”‚   â”œâ”€â”€ pydantic-ai/
â”‚   â”‚   â””â”€â”€ examples/
â”‚   â”‚       â””â”€â”€ main_agent_reference/
â”‚   â”‚           â”œâ”€â”€ models.py              # Data model patterns
â”‚   â”‚           â”œâ”€â”€ providers.py           # LLM provider configuration
â”‚   â”‚           â”œâ”€â”€ research_agent.py      # Multi-agent coordination
â”‚   â”‚           â””â”€â”€ settings.py            # Environment configuration
â”‚   â””â”€â”€ mcp-server/
â”‚       â””â”€â”€ src/                           # MCP server patterns
â”œâ”€â”€ PRPs/
â”‚   â”œâ”€â”€ EXAMPLE_multi_agent_prp.md         # Multi-agent implementation example
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ prp_base.md                    # PRP template structure
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ unit/                              # Testing patterns
â”œâ”€â”€ CLAUDE.md                              # Development guidelines
â”œâ”€â”€ INITIAL.md                             # Requirements specification
â””â”€â”€ pyproject.toml                         # Project configuration
```

### Desired Codebase tree with files to be added
```bash
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py                        # Package initialization
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”œâ”€â”€ __init__.py                    # Orchestrator package init
â”‚   â”‚   â”œâ”€â”€ workflow_analyzer.py           # Analyze projects and generate agent workflows
â”‚   â”‚   â”œâ”€â”€ agent_factory.py               # Generate specialized agents dynamically
â”‚   â”‚   â”œâ”€â”€ context_manager.py             # Persistent context storage and versioning
â”‚   â”‚   â”œâ”€â”€ handoff_coordinator.py         # Manage agent transitions and handoffs
â”‚   â”‚   â””â”€â”€ quality_gates.py               # Validation checkpoints between stages
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py                    # Agents package init
â”‚   â”‚   â”œâ”€â”€ base_agent.py                  # Base agent class with common functionality
â”‚   â”‚   â”œâ”€â”€ discovery_agent.py             # Market research and requirement gathering
â”‚   â”‚   â”œâ”€â”€ specification_agent.py         # Technical requirements and architecture
â”‚   â”‚   â”œâ”€â”€ design_agent.py                # UI/UX design and prototyping
â”‚   â”‚   â”œâ”€â”€ implementation_agent.py        # Code generation and security compliance
â”‚   â”‚   â”œâ”€â”€ validation_agent.py            # QA testing and performance validation
â”‚   â”‚   â””â”€â”€ deployment_agent.py            # Release coordination and monitoring
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py                    # Tools package init
â”‚   â”‚   â”œâ”€â”€ github_integration.py          # GitHub API integration
â”‚   â”‚   â”œâ”€â”€ figma_integration.py           # Figma API integration
â”‚   â”‚   â”œâ”€â”€ jira_integration.py            # Jira API integration
â”‚   â”‚   â”œâ”€â”€ web_research.py                # Web research capabilities
â”‚   â”‚   â””â”€â”€ security_scanner.py            # Security compliance validation
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py                    # Models package init
â”‚   â”‚   â”œâ”€â”€ workflow_models.py             # Workflow and orchestration models
â”‚   â”‚   â”œâ”€â”€ agent_models.py                # Agent communication models
â”‚   â”‚   â”œâ”€â”€ context_models.py              # Context preservation models
â”‚   â”‚   â””â”€â”€ integration_models.py          # External tool integration models
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py                    # Config package init
â”‚   â”‚   â””â”€â”€ settings.py                    # Comprehensive environment configuration
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py                    # Utils package init
â”‚       â”œâ”€â”€ logging_config.py              # Structured logging configuration
â”‚       â””â”€â”€ validation_helpers.py          # Common validation utilities
â”œâ”€â”€ cli/
â”‚   â”œâ”€â”€ __init__.py                        # CLI package init
â”‚   â”œâ”€â”€ main.py                            # Primary CLI interface
â”‚   â”œâ”€â”€ dashboard.py                       # Progress visualization and control
â”‚   â””â”€â”€ commands/
â”‚       â”œâ”€â”€ __init__.py                    # Commands package init
â”‚       â”œâ”€â”€ workflow.py                    # Workflow management commands
â”‚       â”œâ”€â”€ agents.py                      # Agent management commands
â”‚       â””â”€â”€ integrations.py               # Integration setup commands
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py                        # Tests package init
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_orchestrator/             # Orchestrator component tests
â”‚   â”‚   â”œâ”€â”€ test_agents/                   # Individual agent tests
â”‚   â”‚   â”œâ”€â”€ test_tools/                    # Tool integration tests
â”‚   â”‚   â””â”€â”€ test_models/                   # Data model validation tests
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_workflows/                # End-to-end workflow tests
â”‚   â”‚   â”œâ”€â”€ test_handoffs/                 # Agent handoff tests
â”‚   â”‚   â””â”€â”€ test_external_integrations/    # External tool integration tests
â”‚   â””â”€â”€ fixtures/
â”‚       â”œâ”€â”€ sample_projects.py             # Test project definitions
â”‚       â””â”€â”€ mock_responses.py              # Mock external API responses
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md                    # System architecture documentation
â”‚   â”œâ”€â”€ agent_specifications.md            # Individual agent capabilities
â”‚   â””â”€â”€ integration_guide.md               # External tool setup guide
â”œâ”€â”€ .env.example                           # Environment variables template
â”œâ”€â”€ requirements.txt                       # Updated dependencies
â””â”€â”€ README.md                              # Comprehensive setup and usage guide
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: Pydantic AI requires async throughout - no sync functions in async context
# CRITICAL: Context preservation requires careful state management to avoid drift
# CRITICAL: Agent handoffs must pass usage tracking for token management
# CRITICAL: External API integrations have varying rate limits and authentication methods
# CRITICAL: Quality gates must be non-blocking for parallel execution paths
# CRITICAL: Security compliance scanning must run on ALL generated code
# CRITICAL: GitHub API requires proper authentication and webhook handling
# CRITICAL: Figma API has strict CORS policies and token refresh requirements
# CRITICAL: Jira API uses OAuth2 flow with specific scope requirements
# CRITICAL: Multi-agent coordination requires deadlock prevention mechanisms
# CRITICAL: Context versioning must support rollback without data loss
# CRITICAL: Resource allocation must prevent agent conflicts in parallel execution
```

## Implementation Blueprint

### Data models and structure

Create comprehensive data models for agent orchestration, context preservation, and external integrations.

```python
# workflow_models.py - Core orchestration structures
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Union
from datetime import datetime
from enum import Enum

class WorkflowStage(str, Enum):
    DISCOVERY = "discovery"
    SPECIFICATION = "specification"
    DESIGN = "design"
    IMPLEMENTATION = "implementation"
    VALIDATION = "validation"
    DEPLOYMENT = "deployment"

class AgentType(str, Enum):
    DISCOVERY = "discovery"
    SPECIFICATION = "specification"
    DESIGN = "design"
    IMPLEMENTATION = "implementation"
    VALIDATION = "validation"
    DEPLOYMENT = "deployment"

class WorkflowNode(BaseModel):
    id: str = Field(..., description="Unique node identifier")
    stage: WorkflowStage = Field(..., description="Workflow stage")
    agent_type: AgentType = Field(..., description="Required agent type")
    dependencies: List[str] = Field(default_factory=list, description="Node dependencies")
    parallel_eligible: bool = Field(False, description="Can run in parallel")
    quality_gates: List[str] = Field(default_factory=list, description="Required validations")

class ProjectWorkflow(BaseModel):
    project_id: str = Field(..., description="Unique project identifier")
    description: str = Field(..., description="Project description")
    nodes: List[WorkflowNode] = Field(..., description="Workflow nodes")
    current_stage: Optional[WorkflowStage] = Field(None, description="Current active stage")
    status: str = Field("initialized", description="Workflow status")
    created_at: datetime = Field(default_factory=datetime.now)

# context_models.py - Context preservation structures
class ContextSnapshot(BaseModel):
    snapshot_id: str = Field(..., description="Unique snapshot identifier")
    project_id: str = Field(..., description="Associated project")
    stage: WorkflowStage = Field(..., description="Workflow stage")
    data: Dict[str, Any] = Field(..., description="Context data")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")
    created_at: datetime = Field(default_factory=datetime.now)
    version: int = Field(1, description="Context version number")

class HandoffPackage(BaseModel):
    source_agent: AgentType = Field(..., description="Source agent type")
    target_agent: AgentType = Field(..., description="Target agent type")
    context: ContextSnapshot = Field(..., description="Preserved context")
    artifacts: List[Dict[str, Any]] = Field(default_factory=list, description="Stage deliverables")
    quality_score: float = Field(0.0, ge=0.0, le=1.0, description="Quality validation score")
    approved: bool = Field(False, description="Manual approval status")
```

### List of tasks to be completed to fulfill the PRP in the order they should be completed

```yaml
Task 1: Setup Core Infrastructure and Configuration
CREATE src/config/settings.py:
  - PATTERN: Follow use-cases/pydantic-ai/examples/main_agent_reference/settings.py
  - Add comprehensive environment variable management
  - Include external API configurations (GitHub, Figma, Jira)
  - Add orchestration-specific settings (timeouts, concurrency limits)

CREATE .env.example:
  - Include all required environment variables with descriptions
  - Follow CLAUDE.md patterns for environment configuration
  - Add external tool API keys and endpoints

Task 2: Implement Data Models and Validation
CREATE src/models/:
  - PATTERN: Follow use-cases/pydantic-ai/examples/main_agent_reference/models.py
  - Implement workflow_models.py with orchestration structures
  - Implement context_models.py for state preservation
  - Implement agent_models.py for inter-agent communication
  - Implement integration_models.py for external tool data

Task 3: Build Context Management System
CREATE src/orchestrator/context_manager.py:
  - PATTERN: Use persistent storage with versioning capabilities
  - Implement context snapshots with rollback functionality
  - Add context drift detection and correction mechanisms
  - Include memory optimization for large contexts

Task 4: Develop Workflow Analysis Engine
CREATE src/orchestrator/workflow_analyzer.py:
  - PATTERN: Analyze project descriptions using LLM reasoning
  - Generate appropriate agent sequences based on complexity
  - Identify parallel execution opportunities
  - Create dependency graphs for optimal scheduling

Task 5: Implement Agent Factory System
CREATE src/orchestrator/agent_factory.py:
  - PATTERN: Follow Pydantic AI agent creation patterns
  - Generate specialized agents dynamically based on requirements
  - Configure agent dependencies and tool access
  - Implement agent lifecycle management

Task 6: Build Base Agent Architecture
CREATE src/agents/base_agent.py:
  - PATTERN: Follow use-cases/pydantic-ai/examples/main_agent_reference/research_agent.py
  - Implement common agent functionality (logging, error handling)
  - Add context awareness and handoff capabilities
  - Include tool registration and dependency injection

Task 7: Implement Specialized Agents
CREATE src/agents/[agent_type]_agent.py for each:
  - PATTERN: Use base_agent.py as foundation
  - Implement stage-specific tools and capabilities
  - Add quality validation for deliverables
  - Include external tool integrations where appropriate

Task 8: Develop Handoff Coordination System
CREATE src/orchestrator/handoff_coordinator.py:
  - PATTERN: Implement structured handoff protocols
  - Ensure context preservation across transitions
  - Add validation checkpoints and approval workflows
  - Include rollback capabilities for failed handoffs

Task 9: Implement Quality Gates and Validation
CREATE src/orchestrator/quality_gates.py:
  - PATTERN: Executable validation checks for each stage
  - Implement automated quality scoring
  - Add manual approval workflow triggers
  - Include security compliance validation

Task 10: Build External Tool Integrations
CREATE src/tools/:
  - PATTERN: Async tool implementations with proper error handling
  - Implement GitHub API integration for repository management
  - Implement Figma API integration for design synchronization
  - Implement Jira API integration for project tracking
  - Add web research and security scanning capabilities

Task 11: Develop CLI Interface and Dashboard
CREATE cli/:
  - PATTERN: Follow streaming response patterns from examples
  - Implement workflow initiation and management commands
  - Add real-time progress visualization
  - Include interactive approval and control interfaces

Task 12: Implement Comprehensive Testing Suite
CREATE tests/:
  - PATTERN: Mirror existing test structure with pytest
  - Create unit tests for all components with 80%+ coverage
  - Implement integration tests for end-to-end workflows
  - Add performance tests for parallel execution scenarios
  - Include security and compliance validation tests

Task 13: Add Logging and Monitoring
CREATE src/utils/logging_config.py:
  - PATTERN: Structured logging with correlation IDs
  - Implement performance metrics collection
  - Add error tracking and alerting
  - Include audit logging for compliance

Task 14: Create Documentation and Examples
CREATE docs/:
  - Write comprehensive architecture documentation
  - Create agent specification guides
  - Document external tool setup procedures
  - Include usage examples and best practices
```

### Per task pseudocode

```python
# Task 3: Context Management System
class ContextManager:
    def __init__(self, storage_backend: str = "local"):
        # PATTERN: Persistent storage with versioning
        self.storage = self._init_storage(storage_backend)
        self.version_tracker = {}
    
    async def create_snapshot(
        self, 
        project_id: str, 
        stage: WorkflowStage, 
        data: Dict[str, Any]
    ) -> ContextSnapshot:
        # CRITICAL: Validate data integrity before storage
        validated_data = self._validate_context_data(data)
        
        # Generate unique snapshot ID
        snapshot_id = f"{project_id}_{stage}_{int(time.time())}"
        
        # Create versioned snapshot
        version = self._get_next_version(project_id, stage)
        snapshot = ContextSnapshot(
            snapshot_id=snapshot_id,
            project_id=project_id,
            stage=stage,
            data=validated_data,
            version=version
        )
        
        # PATTERN: Atomic storage with rollback capability
        await self.storage.store_snapshot(snapshot)
        return snapshot

# Task 8: Handoff Coordination
class HandoffCoordinator:
    async def execute_handoff(
        self,
        source_agent: Agent,
        target_agent: Agent,
        context: ContextSnapshot,
        quality_threshold: float = 0.8
    ) -> HandoffPackage:
        # PATTERN: Structured validation before handoff
        quality_score = await self._validate_deliverables(
            source_agent, context
        )
        
        if quality_score < quality_threshold:
            # CRITICAL: Block handoff if quality insufficient
            raise HandoffValidationError(
                f"Quality score {quality_score} below threshold {quality_threshold}"
            )
        
        # PATTERN: Context transformation for target agent
        transformed_context = await self._transform_context(
            context, source_agent.type, target_agent.type
        )
        
        # Create handoff package with preserved context
        handoff_package = HandoffPackage(
            source_agent=source_agent.type,
            target_agent=target_agent.type,
            context=transformed_context,
            quality_score=quality_score
        )
        
        # CRITICAL: Pass usage tracking for token management
        await target_agent.initialize(handoff_package, usage=source_agent.usage)
        
        return handoff_package
```

### Integration Points
```yaml
ENVIRONMENT:
  - add to: .env
  - vars: |
      # Core Configuration
      APP_ENV=development
      DEBUG=false
      LOG_LEVEL=INFO
      
      # LLM Configuration
      LLM_PROVIDER=openai
      LLM_API_KEY=sk-...
      LLM_MODEL=gpt-4
      LLM_BASE_URL=https://api.openai.com/v1
      
      # External Tool APIs
      GITHUB_TOKEN=ghp_...
      FIGMA_ACCESS_TOKEN=figd_...
      JIRA_API_TOKEN=ATATT3x...
      JIRA_BASE_URL=https://your-domain.atlassian.net
      
      # Orchestration Settings
      MAX_PARALLEL_AGENTS=5
      CONTEXT_RETENTION_DAYS=30
      QUALITY_GATE_TIMEOUT=300
      
DEPENDENCIES:
  - Update requirements.txt with:
    - pydantic-ai>=0.0.13
    - pydantic>=2.0.0
    - pydantic-settings>=2.0.0
    - httpx>=0.24.0
    - asyncio>=3.4.3
    - python-dotenv>=1.0.0
    - structlog>=23.1.0
    - rich>=13.4.0
    - pytest>=7.4.0
    - pytest-asyncio>=0.21.0

DATABASE:
  - Context storage: SQLite for development, PostgreSQL for production
  - Schema versioning with Alembic migrations
  - Index optimization for context queries

SECURITY:
  - API key encryption at rest
  - Audit logging for all agent actions
  - Input validation and sanitization
  - Generated code security scanning
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
ruff check src/ cli/ tests/ --fix     # Auto-fix style issues
mypy src/ cli/                        # Type checking
black src/ cli/ tests/                # Code formatting

# Expected: No errors. If errors, READ and fix systematically.
```

### Level 2: Unit Tests
```python
# test_orchestrator/test_workflow_analyzer.py
async def test_project_analysis():
    """Test workflow generation from project description"""
    analyzer = WorkflowAnalyzer()
    description = "Build a social media app for dog owners"
    
    workflow = await analyzer.analyze_project(description)
    
    assert workflow.project_id
    assert len(workflow.nodes) >= 3  # At minimum: discovery, design, implementation
    assert WorkflowStage.DISCOVERY in [node.stage for node in workflow.nodes]

async def test_parallel_identification():
    """Test identification of parallel execution opportunities"""
    analyzer = WorkflowAnalyzer()
    workflow = await analyzer.analyze_project("Complex e-commerce platform")
    
    parallel_nodes = [node for node in workflow.nodes if node.parallel_eligible]
    assert len(parallel_nodes) > 0  # Should identify parallel opportunities

# test_orchestrator/test_context_manager.py
async def test_context_preservation():
    """Test context snapshot creation and retrieval"""
    manager = ContextManager()
    test_data = {"requirements": ["feature1", "feature2"], "architecture": "microservices"}
    
    snapshot = await manager.create_snapshot("test_project", WorkflowStage.DISCOVERY, test_data)
    retrieved = await manager.get_snapshot(snapshot.snapshot_id)
    
    assert retrieved.data == test_data
    assert retrieved.stage == WorkflowStage.DISCOVERY

async def test_context_rollback():
    """Test rollback functionality"""
    manager = ContextManager()
    
    # Create multiple versions
    v1 = await manager.create_snapshot("test", WorkflowStage.DISCOVERY, {"version": 1})
    v2 = await manager.create_snapshot("test", WorkflowStage.DISCOVERY, {"version": 2})
    
    # Rollback to v1
    rolled_back = await manager.rollback_to_snapshot(v1.snapshot_id)
    assert rolled_back.data["version"] == 1

# test_agents/test_handoff_coordination.py
async def test_agent_handoff():
    """Test successful agent handoff with context preservation"""
    coordinator = HandoffCoordinator()
    
    # Mock agents and context
    discovery_agent = create_mock_agent(AgentType.DISCOVERY)
    spec_agent = create_mock_agent(AgentType.SPECIFICATION)
    test_context = create_test_context()
    
    handoff = await coordinator.execute_handoff(
        discovery_agent, spec_agent, test_context
    )
    
    assert handoff.source_agent == AgentType.DISCOVERY
    assert handoff.target_agent == AgentType.SPECIFICATION
    assert handoff.quality_score >= 0.8

async def test_quality_gate_enforcement():
    """Test quality gate prevents poor handoffs"""
    coordinator = HandoffCoordinator()
    low_quality_context = create_low_quality_context()
    
    with pytest.raises(HandoffValidationError):
        await coordinator.execute_handoff(discovery_agent, spec_agent, low_quality_context)
```

```bash
# Run tests iteratively until passing:
pytest tests/ -v --cov=src --cov=cli --cov-report=term-missing
pytest tests/integration/ -v -m "not slow"

# If failing: Debug specific failures, fix code, re-run
```

### Level 3: Integration Test
```bash
# Test full workflow execution
python -m cli.main init-workflow "Build a task management app"

# Expected interaction:
# ğŸš€ Analyzing project: "Build a task management app"
# ğŸ“Š Generated workflow with 5 stages
# ğŸ¤– Starting Discovery Agent...
# âœ… Discovery completed (Quality: 0.92)
# ğŸ”„ Handoff to Specification Agent...
# ğŸ¤– Starting Specification Agent...
# ... (continue through all stages)

# Test parallel execution
python -m cli.main workflow --project-id <id> --enable-parallel

# Expected:
# ğŸ”€ Parallel execution enabled
# ğŸ¤– Starting Design Agent (parallel)
# ğŸ¤– Starting Implementation Agent (parallel)
# âš¡ Quality gates coordinating parallel streams

# Test external integrations
python -m cli.main test-integrations

# Expected:
# âœ… GitHub API connection successful
# âœ… Figma API connection successful  
# âœ… Jira API connection successful
# ğŸ“Š All integrations ready for workflow execution
```

## Final Validation Checklist  
- [ ] All tests pass: `pytest tests/ -v --cov=src --cov-report=term-missing`
- [ ] No linting errors: `ruff check src/ cli/ tests/`
- [ ] No type errors: `mypy src/ cli/`
- [ ] Workflow generation works for various project types
- [ ] Context preservation maintains 99%+ accuracy across handoffs
- [ ] Quality gates prevent faulty stage transitions
- [ ] Parallel execution reduces development time significantly
- [ ] External tool integrations sync artifacts correctly
- [ ] Security scanning validates all generated code
- [ ] CLI provides intuitive workflow management
- [ ] Error cases handled gracefully with proper rollback
- [ ] Documentation covers all setup and usage scenarios
- [ ] Performance meets scalability requirements

---

## Anti-Patterns to Avoid
- âŒ Don't use sync functions in async agent context
- âŒ Don't skip context validation before handoffs
- âŒ Don't ignore quality gate failures
- âŒ Don't allow parallel agents to conflict on shared resources
- âŒ Don't hardcode external API endpoints or credentials
- âŒ Don't commit sensitive configuration or API keys
- âŒ Don't bypass security scanning for generated code
- âŒ Don't ignore agent orchestration deadlock scenarios
- âŒ Don't skip audit logging for compliance requirements
- âŒ Don't assume external APIs are always available

## Confidence Score: 8.5/10

High confidence due to:
- Clear understanding of multi-agent orchestration patterns from Pydantic AI
- Proven context preservation techniques from Anthropic's research system
- Existing codebase provides solid foundation and patterns to follow
- Comprehensive external tool integration documentation available
- Well-defined validation gates ensure progressive success

Minor uncertainty areas:
- Complex parallel execution coordination may require iterative refinement
- External API rate limit handling needs careful implementation
- Large-scale context management performance optimization may need tuning

The comprehensive context, proven patterns, and systematic validation approach provide strong foundation for successful one-pass implementation.